apiVersion: v1
kind: Template
metadata:
  name: eph-spark-app-single-job

objects:

- apiVersion: v1
  kind: ConfigMap
  metadata:
    name: oshinko-py36-conf
  data:
    workercount: "${NUM_SPARK_WORKERS}"
    sparkimage: radanalyticsio/openshift-spark-py36:latest

- apiVersion: batch/v1
  kind: Job
  metadata:
    name: ${APPLICATION_NAME}
    labels:
      deploymentConfig: ${APPLICATION_NAME}
      app: ${APPLICATION_NAME}
  spec:
    replicas: 1
    selector:
      deploymentConfig: ${APPLICATION_NAME}
    strategy:
      type: Rolling
    template:
      metadata:
        labels:
          deploymentConfig: ${APPLICATION_NAME}
          app: ${APPLICATION_NAME}
      spec:
        containers:
        - env:
          - name: DRIVER_HOST
            value: ${APPLICATION_NAME}-headless
          - name: OSHINKO_CLUSTER_NAME
            value: ${OSHINKO_CLUSTER_NAME}
          - name: APP_ARGS
            value: ${APP_ARGS}
          - name: SPARK_OPTIONS
            value: ${SPARK_OPTIONS}
          - name: OSHINKO_DEL_CLUSTER
            value: ${OSHINKO_DEL_CLUSTER}
          - name: APP_EXIT
            value: ${APP_EXIT}
          - name: OSHINKO_NAMED_CONFIG
            value: ${OSHINKO_NAMED_CONFIG}
          - name: OSHINKO_SPARK_DRIVER_CONFIG
            value: ${OSHINKO_SPARK_DRIVER_CONFIG}
          - name: DH_CEPH_KEY
            value: ${DH_CEPH_KEY}
          - name: DH_CEPH_SECRET
            value: ${DH_CEPH_SECRET}
          - name: DH_CEPH_HOST
            value: ${DH_CEPH_HOST}
          - name: DH_CEPH_BUCKET
            value: ${DH_CEPH_BUCKET}
          - name: PROM_METRIC_NAME
            value: ${PROM_METRIC_NAME}
          - name: SPARK_DEFAULT
            value: ${SPARK_DEFAULT}
          - name: METRIC_TIME_FRAME
            value: ${METRIC_TIME_FRAME}
          - name: BEGIN_TIMESTAMP
            value: ${BEGIN_TIMESTAMP}
          - name: END_TIMESTAMP
            value: ${END_TIMESTAMP}
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          image: ${APPLICATION_NAME}
          imagePullPolicy: IfNotPresent
          name: ${APPLICATION_NAME}
          resources: {}
          terminationMessagePath: /dev/termination-log
          volumeMounts:
          - mountPath: /etc/podinfo
            name: podinfo
            readOnly: false
        dnsPolicy: ClusterFirst
        restartPolicy: OnFailure
        serviceAccount: oshinko
        volumes:
        - downwardAPI:
            items:
            - fieldRef:
                fieldPath: metadata.labels
              path: labels
          name: podinfo
    triggers:
    - imageChangeParams:
        automatic: true
        containerNames:
          - ${APPLICATION_NAME}
        from:
          kind: ImageStreamTag
          name: ${APPLICATION_NAME}:latest
      type: ImageChange
    - type: ConfigChange

parameters:
- description: 'The name to use for the buildconfig, imagestream and deployment components'
  from: 'python-spark-[a-z0-9]{4}'
  generate: expression
  name: APPLICATION_NAME
  required: true
- description: The URL of the repository with your application source code
  displayName: Git Repository URL
  name: GIT_URI
- description: Optional branch, tag or commit
  displayName: Git Reference
  name: GIT_REF
- description: Git sub-directory path
  name: CONTEXT_DIR
- description: The name of the main py file to run. If this is not specified and there is a single py file at top level of the git respository, that file will be chosen.
  name: APP_FILE
- description: Command line arguments to pass to the Spark application
  name: APP_ARGS
- description: List of additional Spark options to pass to spark-submit (for exmaple --conf property=value --conf property=value). Note, --master and --class are set by the launcher and should not be set here
  name: SPARK_OPTIONS
- description: The name of the Spark cluster to run against. The cluster will be created if it does not exist, and a random cluster name will be chosen if this value is left blank.
  name: OSHINKO_CLUSTER_NAME
- description: The name of a stored cluster configuration to use if a cluster is created, default is 'default'.
  name: OSHINKO_NAMED_CONFIG
  value: oshinko-py36-conf
- description: The name of a configmap to use for the Spark configuration of the driver. If this configmap is empty the default Spark configuration will be used.
  name: OSHINKO_SPARK_DRIVER_CONFIG
- description: If a cluster is created on-demand, delete the cluster when the application finishes if this option is set to 'true'
  name: OSHINKO_DEL_CLUSTER
  required: true
  value: 'true'
- description: Number of Spark Workers to spawn
  name: NUM_SPARK_WORKERS
  required: false
  value: "2"
- description: S3 storage access key
  name: DH_CEPH_KEY
  required: true
- description: S3 storage secret key
  name: DH_CEPH_SECRET
  required: true
- description: S3 storage endpoint url
  name: DH_CEPH_HOST
  required: true
- description: S3 storage bucket name
  name: DH_CEPH_BUCKET
  required: true
- description: Metric name on which the analysis is to be performed on
  name: PROM_METRIC_NAME
  required: true
- description: Time frame of data to perform Forecasting
  name: METRIC_TIME_FRAME
  required: true
- description: Exit from the application (It will keep respawning)
  name: APP_EXIT
  required: false
  value: "true"
- description: Beginning of Data period used to train the model
  name: BEGIN_TIMESTAMP
  required: false
- description: End of Data period used to train the model
  name: END_TIMESTAMP
  required: false
