apiVersion: v1
kind: Template
metadata:
  name: eph-spark-app-single-job

objects:

  - apiVersion: v1
    kind: Template
    labels:
      application: oshinko-python36-spark
      createdBy: template-oshinko-python36-spark-build-dc
    metadata:
      annotations:
        description: Create a buildconfig, imagestream and deploymentconfig using source-to-image and Python 3.6 Spark source files hosted in git
        openshift.io/display-name: Apache Spark Python 3.6
      name: oshinko-python36-spark-build-dc
    objects:
    - apiVersion: v1
      kind: ImageStream
      metadata:
        name: ${APPLICATION_NAME}
        labels:
          app: ${APPLICATION_NAME}
      spec:
        dockerImageRepository: ${APPLICATION_NAME}
        tags:
        - name: latest
        lookupPolicy:
          local: true

    - apiVersion: v1
      kind: BuildConfig
      metadata:
        name: ${APPLICATION_NAME}
        labels:
          app: ${APPLICATION_NAME}
      spec:
        lookupPolicy:
          local: true
        output:
          to:
            kind: ImageStreamTag
            name: ${APPLICATION_NAME}:latest
        source:
          contextDir: ${CONTEXT_DIR}
          git:
            ref: ${GIT_REF}
            uri: ${GIT_URI}
          type: Git
        strategy:
          sourceStrategy:
            env:
            - name: APP_FILE
              value: ${APP_FILE}
            - name: GIT_SSL_NO_VERIFY
              value: 'true'
            forcePull: true
            from:
              kind: DockerImage
              name: radanalyticsio/radanalytics-pyspark-py36:latest
          type: Source
        triggers:
        - imageChange: {}
          type: ImageChange
        - type: ConfigChange
        - github:
            secret: ${APPLICATION_NAME}
          type: GitHub
        - generic:
            secret: ${APPLICATION_NAME}
          type: Generic

    - apiVersion: v1
      kind: Service
      metadata:
        name: ${APPLICATION_NAME}
        labels:
          app: ${APPLICATION_NAME}
      spec:
        ports:
        - name: 8080-tcp
          port: 8080
          protocol: TCP
          targetPort: 8080
        selector:
          deploymentConfig: ${APPLICATION_NAME}
    - apiVersion: v1
      kind: Service
      metadata:
        name: ${APPLICATION_NAME}-headless
        labels:
          app: ${APPLICATION_NAME}
      spec:
        clusterIP: None
        ports:
        - name: driver-rpc-port
          port: 7078
          protocol: TCP
          targetPort: 7078
        - name: blockmanager
          port: 7079
          protocol: TCP
          targetPort: 7079
        selector:
          deploymentConfig: ${APPLICATION_NAME}

parameters:
- description: 'The name to use for the buildconfig, imagestream and deployment components'
  from: 'python-spark-[a-z0-9]{4}'
  generate: expression
  name: APPLICATION_NAME
  required: true
- description: The URL of the repository with your application source code
  displayName: Git Repository URL
  name: GIT_URI
- description: Optional branch, tag or commit
  displayName: Git Reference
  name: GIT_REF
- description: Git sub-directory path
  name: CONTEXT_DIR
- description: The name of the main py file to run. If this is not specified and there is a single py file at top level of the git respository, that file will be chosen.
  name: APP_FILE
- description: Command line arguments to pass to the Spark application
  name: APP_ARGS
- description: List of additional Spark options to pass to spark-submit (for exmaple --conf property=value --conf property=value). Note, --master and --class are set by the launcher and should not be set here
  name: SPARK_OPTIONS
- description: The name of the Spark cluster to run against. The cluster will be created if it does not exist, and a random cluster name will be chosen if this value is left blank.
  name: OSHINKO_CLUSTER_NAME
- description: The name of a stored cluster configuration to use if a cluster is created, default is 'default'.
  name: OSHINKO_NAMED_CONFIG
  value: oshinko-py36-conf
- description: The name of a configmap to use for the Spark configuration of the driver. If this configmap is empty the default Spark configuration will be used.
  name: OSHINKO_SPARK_DRIVER_CONFIG
- description: If a cluster is created on-demand, delete the cluster when the application finishes if this option is set to 'true'
  name: OSHINKO_DEL_CLUSTER
  required: true
  value: 'true'
